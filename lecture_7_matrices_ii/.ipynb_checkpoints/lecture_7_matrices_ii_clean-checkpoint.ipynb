{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Computational Linear Algebra II \n",
    "\n",
    "## Data Science Fundamentals\n",
    "\n",
    "## Linear systems, inversion and matrix decompositions\n",
    "----\n",
    "##### DSF - University of Glasgow - Chris McCaig - 2020/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "By the end of this unit you should know:\n",
    "\n",
    "* the basic notation for matrices\n",
    "* the view of matrices as linear maps\n",
    "* how basic geometric transforms are implemented using matrices\n",
    "* how matrix multiplication is defined and its algebraic properties\n",
    "* the basic anatomy of matrices  how discrete problems can be modelled using continuous mathematics, i.e. using matrices\n",
    "    * how graphs can be represented as matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\vec}[1]{{\\bf #1} } \n",
    "\\newenvironment{examinable}{}{\\ \\ [\\spadesuit]}\n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "%\\begin{examinable}\n",
    "%\\vec{x}\n",
    "%\\in\n",
    "%\\real\n",
    "%\\end{examinable}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.HTML(\"\"\"\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T12:04:11.222183Z",
     "start_time": "2020-10-08T12:04:10.494951Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from jhwutils.matrices import print_matrix, show_matrix_effect\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.rc('figure', figsize=(8.0, 4.0), dpi=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices and linear operators\n",
    "\n",
    "## Uses of matrices\n",
    "We have seen that (real) vectors represent elements of a vector space as 1D arrays of real numbers (and implemented as ndarrays of floats). \n",
    "\n",
    "Matrices represent **linear maps** as 2D arrays of reals; $\\real^{m\\times n}$.\n",
    "\n",
    "* Vectors represent \"points in space\"\n",
    "* Matrices represent *operations* that do things to those points in space. \n",
    "\n",
    "The operations represented by matrices are a particular class of functions on vectors -- \"rigid\" transformations. Matrices are a very compact way of writing down these operations.\n",
    "\n",
    "### Operations with matrices\n",
    "There are many things we can do with matrices:\n",
    "\n",
    "* They can be added and subtracted $C=A+B$ \n",
    "    *  $(\\real^{n\\times m},\\real^{n\\times m}) \\rightarrow \\real^{n\\times m}$\n",
    "* They can be scaled with a scalar $C = sA$\n",
    "    * $(\\real^{n\\times m},\\real) \\rightarrow \\real^{n\\times m}$\n",
    "* They can be transposed $B = A^T$; this exchanges rows and columns\n",
    "    * $\\real^{n\\times m} \\rightarrow \\real^{m\\times n}$\n",
    "* They can be *applied to vectors* $\\vec{y} = A\\vec{x}$; this **applies** a matrix to a vector.\n",
    "    * $(\\real^{n\\times m}, \\real^{m}) \\rightarrow \\real^{n}$\n",
    "* They can be *multiplied together* $C = AB$; this **composes** the effect of two matrices \n",
    "    * $(\\real^{p\\times q}, \\real^{q\\times r})\\rightarrow \\real^{p\\times r}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Intro to matrix notation\n",
    "We write matrices as a capital letter: \n",
    "\n",
    "$$A \\in \\real^{n \\times m}=  \\begin{bmatrix}\n",
    "a_{1,1}  & a_{1,2} & \\dots & a_{1,m}  \\\\\n",
    "a_{2,1}  & a_{2,2}  & \\dots & a_{2,m}  \\\\\n",
    "\\dots \\\\\n",
    "a_{n,1} + & a_{n,2}  & \\dots & a_{n,m} \\\\\n",
    "\\end{bmatrix},\\  a_{i,j}\\in \\real$$\n",
    "\n",
    "(although we don't usually write matrices with capital letters in code -- they follow the normal rules for variable naming like any other value)\n",
    "\n",
    "A matrix with dimension $n \\times m$ has $n$ rows and $m$ columns (remember this order -- it is important!). Each element of the matrix $A$ is written as $a_{i,j}$ for the $i$th row and $j$th column.\n",
    "\n",
    "Matrices correspond to the 2D arrays / rank-2 tensors we are familiar with from earlier. But they have a very rich mathematical structure which makes them of key importance in computational methods. *Remember to distinguish 2D arrays from the mathematical concept of matrices. Matrices (in the linear algebra sense) are represented by 2D arrays, as real numbers are represented by floating-point numbers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:41:40.772185Z",
     "start_time": "2020-10-01T13:41:40.765204Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print_matrix(\"A\", a)\n",
    "# note that code indexes from 0, whereas mathematical notation indexes from 1!\n",
    "\n",
    "print_matrix(\"A_{1,3}\", a[0,2]) # index row i=0, column j=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices as maps\n",
    "We saw vectors as **points in space**. Matrices represent **linear maps** -- these are functions applied to vectors which outputs vectors. In the standard notation, matrices are *applied* to vectors by multiplying them:\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$ A{\\bf x} = f({\\vec x}) $$\n",
    "</div>\n",
    "\n",
    "This is equivalent to applying some function $f({\\vec x})$ to the vectors. Matrices represent functions mapping vectors to vectors in a very compact form, and they capture a special set of functions that preserve important properties of the vectors they act on. We'll see how matrix-vector multiplication is defined algorithmically shortly.\n",
    "\n",
    "#### Effect of matrix transform\n",
    "Specifically, a $n \\times m$ matrix $A$ represents a function $f({\\bf x})$ taking $m$ dimensional vectors to $n$ dimensional vectors, ($\\real^m \\rightarrow \\real^n$) such that all straight lines remain straight and all parallel lines remain parallel, and the origin does not move (i.e. that the zero vector $\\vec{0^m} [0,0,\\dots,0]\\rightarrow \\vec{0^n}[0,0,\\dots,0]$).\n",
    "\n",
    "##### Linearity\n",
    "This is equivalent to saying that:\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$ \n",
    "f(\\vec{x}+\\vec{y}) = f(\\vec{x}) + f(\\vec{y}) \\quad =  A({\\bf x}+{\\bf y}) = A{\\bf x} + A{\\bf y}, \\\\\n",
    "f(c\\vec{x}) = cf(\\vec{x}) \\quad = A(c{\\bf x}) = cA{\\bf x},\n",
    "$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "i.e. the transform of the sum of two vectors is the same as the sum of the transform of two vectors, and the transform of a scalar multiple of a vector is the same as the scalar multiple of the transform of a vector. This property is **linearity**, and matrices represent **linear maps** or **linear functions**.\n",
    "\n",
    "> Anything which is linear is easy. Anything which isn't linear is hard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric intuition (cube -> parallelepiped)\n",
    "An intuitive way of understanding matrix operations is to consider a matrix to transform a cube of vector space centered on the origin in one space to a **parallelotope** in another space, with the origin staying fixed. This is the *only* kind of transform a matrix can apply.\n",
    "\n",
    "A parallelotope is the generalisation of a parallelogram to any finite dimensional vector space, which has parallel faces but edges which might not be at 90 degrees.\n",
    "\n",
    "<img src=\"imgs/parallel.png\">\n",
    "\n",
    "#### Transforms and projections\n",
    "* A linear map is any function $f$ $R^m \\rightarrow R^n$ which satisfies the linearity requirements.\n",
    "* If the map represented by the matrix is $n\\times n$ then it maps from a vector space onto the *same* vector space (e.g. from $\\real^n \\rightarrow \\real^n$), and it is called a **linear transform**.\n",
    "* If the map has the property $Ax = AAx$ or equivalently $f(x)= f(f(x))$ then the operation is called a **linear projection**; for example, projecting 3D points onto a plane; applying this transform to a set of vectors twice is the same as applying it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it real\n",
    "We will only consider **real matrices** in this course, although the abstract definitions above apply to linear maps across any vector space (e.g complex numbers, finite fields, polynomials).\n",
    "\n",
    "#### Linear maps are representable as matrices\n",
    "*Every linear map of real vectors can be written as a real matrix.* In other words, if there is a function $f(\\vec{x})$ that satisfies the linearity conditions above, it can be expressed as a matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "It's easiest to see the effect of matrix operations in low-dimensional vector spaces. Let's visualise some examples of linear transforms (linear maps $A \\in \\real^{2\\times 2}, \\real^2 \\rightarrow \\real^2$), on the 2D plane. \n",
    "\n",
    "We will take collections of vectors $\\vec{x_1}, \\vec{x_2}, \\dots$ and then apply various matrices to them. We forms the product $A\\vec{x}$, which \"applies\" the matrix to the vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:12.702540Z",
     "start_time": "2020-10-01T13:45:12.495056Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(np.array(\n",
    "        [[1,0], \n",
    "         [0,1]]), suptitle=\"Identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:20.284439Z",
     "start_time": "2020-10-01T13:45:20.080983Z"
    }
   },
   "outputs": [],
   "source": [
    "# uniform scaling\n",
    "show_matrix_effect(np.array(\n",
    "        [[0.5, 0], \n",
    "         [0,   0.5]]), suptitle=\"Uniform scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:29.433491Z",
     "start_time": "2020-10-01T13:45:29.234992Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-uniform scaling\n",
    "show_matrix_effect(np.array(\n",
    "        [[0.5, 0], \n",
    "         [0,   1.0]]), suptitle=\"Non-uniform scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:42.051069Z",
     "start_time": "2020-10-01T13:45:41.845597Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 90 degrees\n",
    "show_matrix_effect(np.array(\n",
    "        [[0, 1], \n",
    "         [-1, 0]]), suptitle=\"Rotate 90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:53.872134Z",
     "start_time": "2020-10-01T13:45:53.663187Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 30 degrees\n",
    "# don't worry about how this matrix is constructed just yet\n",
    "# but observe its effect\n",
    "d30 = np.radians(30)\n",
    "cs = np.cos(d30)\n",
    "ss = np.sin(d30)\n",
    "\n",
    "show_matrix_effect(np.array(\n",
    "        [[cs, ss], \n",
    "         [-ss, cs]]), suptitle=\"Rotate 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:04.237162Z",
     "start_time": "2020-10-01T13:46:04.038168Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 45 degrees, scale by 0.5\n",
    "d30 = np.radians(45)\n",
    "cs = np.cos(d30) * 0.5\n",
    "ss = np.sin(d30) * 0.5\n",
    "\n",
    "show_matrix_effect(np.array([[cs, ss], \n",
    "                             [-ss, cs]]), \"Rotate 45, Scale 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:12.289423Z",
     "start_time": "2020-10-01T13:46:12.093424Z"
    }
   },
   "outputs": [],
   "source": [
    "# flip x\n",
    "show_matrix_effect(np.array([[-1, 0], \n",
    "                             [0, 1]]), \"Flip x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:20.888420Z",
     "start_time": "2020-10-01T13:46:20.687432Z"
    }
   },
   "outputs": [],
   "source": [
    "# shear\n",
    "show_matrix_effect(np.array([[0.15, 0.75], \n",
    "                             [0.5, 0.8]]), \"Shear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:38.018225Z",
     "start_time": "2020-10-01T13:46:37.818232Z"
    }
   },
   "outputs": [],
   "source": [
    "# random!\n",
    "show_matrix_effect(np.random.uniform(-1, 1, (2, 2)), \"Random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "There is an **algebra** of matrices; this is **linear algebra**. In particular, there is a concept of addition of matrices of *equal size*, which is simple elementwise addition:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$  A + B = \\begin{bmatrix}\n",
    "a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots & a_{1,m} + b_{1,m} \\\\\n",
    "a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots & a_{2,m} + b_{2,m} \\\\\n",
    "\\dots \\\\\n",
    "a_{n,1} + b_{n,1} & a_{n,2} + b_{n,2} & \\dots & a_{n,m} + b_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "along with scalar multiplication $cA$, which multiplies each element by $c$.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    " \n",
    "$$  cA = \\begin{bmatrix}\n",
    "ca_{1,1}  & ca_{1,2} & \\dots & ca_{1,m}  \\\\\n",
    "ca_{2,1} & ca_{2,2}  & \\dots & ca_{2,m} \\\\\n",
    "\\dots \\\\\n",
    "ca_{n,1}  & ca_{n,2} & \\dots & ca_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "These correspond exactly to addition and scalar multiplication in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:47:08.886256Z",
     "start_time": "2020-10-01T13:47:08.870300Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.arange(9).reshape(3, 3)\n",
    "b = np.array([[1, 0, 1], \n",
    "              [-1, -1, -1], \n",
    "              [1, -1, 0]])\n",
    "\n",
    "print_matrix(\"A\", a)\n",
    "print_matrix(\"B\", b)\n",
    "print_matrix(\"A+B\", a + b)  # matrix addition\n",
    "print_matrix(\"2A=A+A\", a * 2)  # scalar multiplication\n",
    "print_matrix(\"0.5A\", a * 0.5)  # scalar multiplication\n",
    "print_matrix(\"A-B = A+(-1)B\", a - b)  # equal to (-1) * a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to vectors\n",
    "We can apply a matrix to a vector. We write it as a product $A\\vec{x}$, to mean the matrix $A$ applied to the vector $\\vec{x}$.  This is equivalent to applying the function $f(\\vec{x})$, where $f$ is the corresponding function.\n",
    "\n",
    "If $A$ is $\\real^{n \\times m}$, and $\\vec{x}$ is $\\real^m$, then this will map from an $m$ dimensional vector space to an $n$ dimensional vector space.\n",
    "\n",
    "**All application of a matrix to a vector does is form a weighted sum of the elements of the vector**. This is a linear combination (equivalent to a \"weighted sum\") of the components.\n",
    "\n",
    "In particular, we take each element of $\\vec{x}, x_1, x_2, \\dots, x_m$, multiply it with the corresponding *column* of $A$, and sum these columns together.\n",
    "\n",
    "* Set $\\vec{y}=[0,0,0,\\dots]=0^n$ (the n-dimensional zero vector)\n",
    "* For each column $1\\leq i \\leq m$ in $A$\n",
    "    *    $\\vec{y} = \\vec{y} + x_iA_i$. Note that $x_iA_i$ is scalar times vector, and has $n$ elements. $A_i$ here means the $i$th column of $A$.\n",
    "\n",
    "\n",
    "    1 2     1 \n",
    "    3 4     2\n",
    "    5 6\n",
    "    7 8\n",
    "                 \n",
    "    = 1 * [1,3,5,7] + 2 * [2,4,6,8]\n",
    "    = [1*1+2*2, 1*3+2*5, 1*4+2*6, 1*7+2*8], \n",
    "    = [5, 11, 17, 23]     \n",
    "    \n",
    "    We can use @ to form products of vectors and matrices in Numpy:\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:48:51.872411Z",
     "start_time": "2020-10-01T13:48:51.863437Z"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "x = np.array([1, 2])\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"\\\\bf x\", x)\n",
    "print_matrix(\"A\\\\bf x\", A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:08.826408Z",
     "start_time": "2020-10-01T13:49:08.820424Z"
    }
   },
   "outputs": [],
   "source": [
    "# we'd never do this by hand\n",
    "def apply_matrix_vector(A, x):\n",
    "    y = np.zeros(A.shape[0])\n",
    "    for i in range(A.shape[1]):\n",
    "        y += x[i] * A[:, i]\n",
    "    return y\n",
    "\n",
    "print_matrix(\"A\\\\bf x\", apply_matrix_vector(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:12.367725Z",
     "start_time": "2020-10-01T13:49:12.360773Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "A = np.array([[1, -1, 1], [1, 0, 0], [0, 1, 1]])  # 3x3\n",
    "print_matrix(\"x\", x)  # note: written horizontally, but interpreted vertically!\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"Ax\", A @ x)  # linear transform -- output dimension == input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:24.024261Z",
     "start_time": "2020-10-01T13:49:24.016773Z"
    }
   },
   "outputs": [],
   "source": [
    "B = np.array([[2, -5, 5], [1, 0, 0]])  # 2x3 -- OK\n",
    "print_matrix(\"B\", B)\n",
    "print_matrix(\"Bx\", B @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:34.579093Z",
     "start_time": "2020-10-01T13:49:34.566125Z"
    }
   },
   "outputs": [],
   "source": [
    "### shape error\n",
    "C = np.array([[2, -5], [1, 0], [3, 3]])  # 3x2 -- not OK\n",
    "print_matrix(\"C\", C)\n",
    "print_matrix(\"Cx\", C @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "Multiplication is the interesting matrix operation. Matrix multiplication defines the product $C=AB$, where $A,B,C$ are all matrices.\n",
    "\n",
    "Matrix multiplication is defined such that if $A$ represents linear transform $f(\\vec{x})$ and\n",
    "$B$ represents linear transform $g(\\vec{x})$, then $BA\\vec{x} = g(f(\\vec{x}))$.\n",
    "\n",
    "**Multiplying two matrices is equivalent to composing the linear functions they represent, and it results in a matrix which has that affect.**\n",
    "\n",
    "*Note that the composition of linear maps is read right to left. To apply the transformation $A$, **then** $B$, we form the product $BA$, and so on.*\n",
    "\n",
    "### Multiplication algorithm\n",
    "This gives rise to many important uses of matrices: for example, the product of a scaling matrix and a rotation matrix is a scale-and-rotate matrix. It also places some requirements on the matrices which form a valid product. Multiplication is *only* defined for two matrices $A, B$ if:\n",
    "* $A$ is $p \\times q$ and\n",
    "* $B$ is $q \\times r$.\n",
    "\n",
    "This follows from the definition of multiplication: $A$ represents a map $\\real^q \\rightarrow \\real^p$ and $B$ represents a map $\\real^r \\rightarrow \\real^q$. The output of $A$ must match the dimension of the input of $B$, or the operation is undefined. \n",
    "\n",
    "Matrix multiplication is defined in a slightly surprising way, which is easiest to see in the form of an algorithm:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "If $C=AB$ then $$C_{ij}=\\sum_k a_{ik} b_{kj}$$\n",
    "</div>\n",
    "The element at $C_{ij}$ is the sum of the elementwise product of the $i$th row and the $j$th column, which will be the same size by the requirement above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:12.701063Z",
     "start_time": "2020-10-01T13:58:12.696076Z"
    }
   },
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    p, q_a = a.shape\n",
    "    q_b, r = b.shape\n",
    "    # we can only multiply two matrices if A is p x q and B in q x r\n",
    "    assert q_a == q_b\n",
    "    # the result is a matrix of size p x r\n",
    "    c = np.zeros((p, r))\n",
    "    for i in range(p):\n",
    "        for j in range(r):\n",
    "            # Note that this can be seen as a simple *weighted sum*\n",
    "            # the sum of the ith row of A weighted by the jth column of B\n",
    "            c[i, j] = np.sum(a[i, :] * b[:, j])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:13.896012Z",
     "start_time": "2020-10-01T13:58:13.887036Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, -3]])\n",
    "b = np.array([[1, -1, 1], \n",
    "              [2, -2, 2], \n",
    "              [3, -3, 3]])\n",
    "\n",
    "print_matrix(\"{\\\\bf A}\", a)\n",
    "print_matrix(\"{\\\\bf B}\", b)\n",
    "c = matmul(a,b)\n",
    "print_matrix(\"{\\\\bf A}B\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is of course built in to NumPy, and much more efficient than this algorthim. Matrix multiplication is applied by `np.dot(a,b)` or by the syntax `a @ b`\n",
    "\n",
    "We'll use `a @ b` as the standard syntax for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:32.053691Z",
     "start_time": "2020-10-01T13:58:32.046710Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify that this is the same as the built in matrix multiply\n",
    "c_numpy = np.dot(a, b)\n",
    "print_matrix(\"C_{\\\\text numpy}\", c_numpy)\n",
    "print(np.allclose(c, c_numpy))\n",
    "\n",
    "c_at = a @ b \n",
    "print_matrix(\"C_{\\\\text a @ b}\", a @ b)\n",
    "print(np.allclose(c_at, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time complexity of multiplication\n",
    "Matrix multiplication has, in the general case, of time complexity $O(pqr)$, or for multiplying two square matrices $O(n^3)$. This is apparent from the three nested loops above. However, there are many special forms of matrices for which this complexity can be reduced, such as diagonal, triangular, sparse and banded matrices. We will we see these **special forms** later.\n",
    "\n",
    "There are some accelerated algorithms for general multiplication. The time complexity of all of them is $>O(N^2)$ but $<O(N^3)$. Most accelerated algorithms are impractical for all but the largest matrices because they have enormous constant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Apply matrices to vectors\n",
    "The same algorithm for multiplying two matrices applies to multiplying a matrix by a vector **if we assume a $m$ dimensional vector $\\vec{x} \\in \\real^m$ is represented as a $m \\times 1$ column vector**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_m\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the product $Ax$ is application of the linear map defined by $A$ to vector $\\vec{x}$. $A$ must be of dimension $n\\times m$ for this operation to be defined. If $A$ is $m \\times m$ then it is a **linear transform** (as we defined it above), and the result is another vector of the same dimension.\n",
    "\n",
    "Note: this is a slight abuse of notation. $\\vec{x}$ is a vector, not a matrix, and it is neither a column vector nor a row vector -- it's just an element of a vector space. However, it's convenient to pretend it works like a $m \\times 1$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:59:04.319942Z",
     "start_time": "2020-10-01T13:59:04.313958Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"Ax\", A @ x)\n",
    "# force to a 2D array of m x 1 size\n",
    "print_matrix(\"Ax\", A @ x.reshape(-1, 1))\n",
    "# note this will still be a 2D array in NumPy\n",
    "# and will appear differently when printed. However, it\n",
    "# has the same *semantics* as a 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposition\n",
    "The **transpose** of a matrix $A$ is written $A^T$ and has the same elements, but with the rows and columns exchanged. Many matrix algorithms use transpose in computations.\n",
    "\n",
    "NumPy uses the `A.T` syntax to transpose any array by reversing its stride array, which corresponds to the mathematical transpose for matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:59:35.162985Z",
     "start_time": "2020-10-01T13:59:35.156004Z"
    }
   },
   "outputs": [],
   "source": [
    "### Transpose\n",
    "A = np.array([[2, -5], [1, 0], [3, 3]])\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"A^T\", A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column and row vectors\n",
    "The transpose of a column vector $$\\vec{x}=\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$ is a row vector  $$\\vec{x}^T=\n",
    "\\begin{bmatrix}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\dots & \n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that from our definition of matrix multiplication, the product of a Mx1 with a 1xN vector is an $M \\times N$ matrix. This is the **outer product** of two vectors, every possible combination of their elements:\n",
    "\n",
    "$$\\vec{x} \\otimes \\vec{y} = \\vec{x}^T \\vec{y}$$\n",
    "\n",
    "and the product of a 1xN with an Nx1 vector is a 1x1 matrix; a scalar. This is exactly the **inner product** of two vectors:\n",
    "\n",
    "$$\\vec{x} \\bullet \\vec{y} = \\vec{x}\\vec{y^T} ,$$\n",
    "and is only defined for vectors $\\vec{x}, \\vec{y}$ of the same length.\n",
    "\n",
    "[again, we abuse notation to make it meaningful to \"transpose\" a vector; this is assuming we treat it as a column vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:00:37.362543Z",
     "start_time": "2020-10-01T14:00:37.356559Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3]])\n",
    "y = np.array([[4,5,6]])\n",
    "\n",
    "print_matrix(\"{\\\\bf x}\", x)\n",
    "print_matrix(\"{\\\\bf y}\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:00:52.160410Z",
     "start_time": "2020-10-01T14:00:52.151434Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"{\\\\bf x} \\otimes {\\\\bf y}\", np.outer(x,y))\n",
    "print_matrix(\"{\\\\bf x}^T{\\\\bf y}\", x.T @ y)\n",
    "\n",
    "print_matrix(\"{\\\\bf x} \\\\bullet {\\\\bf y}\", np.inner(x,y))\n",
    "print_matrix(\"{\\\\bf x}{\\\\bf y}^T\", x @ y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composed maps \n",
    "\n",
    "There is a very important property of matrices. If $A$ represents $f(x)$ and $B$ represents $g(x)$, then the product $BA$ represents $g(f(x))$. **Multiplication is composition.** Note carefully the order of operations. $BA\\vec{x} = B(A\\vec{x})$ means do $A$ to $\\vec{x}$, then do $B$ to the result.\n",
    "\n",
    "We can visually verify that composition of matrices by multiplication is the composition of their effects. For example, lets define a nonuniform scaling and a rotation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:52.238648Z",
     "start_time": "2020-10-01T14:01:52.235652Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rotation\n",
    "d30 = np.radians(30)\n",
    "cs = np.cos(d30)\n",
    "ss = np.sin(d30)\n",
    "rot30 = np.array([[cs, ss], [-ss, cs]])\n",
    "\n",
    "## Scaling\n",
    "scale_x = np.array([[1,0], [0, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:53.974142Z",
     "start_time": "2020-10-01T14:01:53.765151Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(rot30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:57.656121Z",
     "start_time": "2020-10-01T14:01:57.461626Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:02:10.086256Z",
     "start_time": "2020-10-01T14:02:09.876816Z"
    }
   },
   "outputs": [],
   "source": [
    "A = scale_x @ rot30 # product of scale and rotate\n",
    "print(A)\n",
    "show_matrix_effect(A, \"Rotate then scale\")   # rotate, then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:02:39.746964Z",
     "start_time": "2020-10-01T14:02:39.535499Z"
    }
   },
   "outputs": [],
   "source": [
    "B = rot30 @ scale_x\n",
    "print(B)\n",
    "show_matrix_effect(B, \"Scale then rotate\")  # scale, then rotate\n",
    "# note: Not the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of transforms\n",
    "Many software operations take advantage of the definition of matrix multiplication as the composition of linear maps. In a graphics processing pipeline, for example, all of the operations to position, scale and orient visible objects are represented as matrix transforms. Multiple operations can be combined into *one single matrix operation*.\n",
    "\n",
    "The desktop UI environment you are uses linear transforms to represent the transformation from data coordinates to screen coordinates. Because multiplication composes transforms, only a single matrix for each object needs to be kept around. (actually for 3D graphics, at least two matrices are kept: one to map 3D -> 3D (the *modelview matrix*) and one to map 3D -> 2D (the *projection matrix*)).\n",
    "\n",
    "Rotating an object by 90 degrees computes product of the current view matrix with a 90 degree rotation matrix, which is then stored in place of the previous view matrix. This means that all rendering just needs to apply to relevant matrix to the geometry data to get the pixel coordinates to perform the rendering.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commutativity\n",
    "The order of multiplication is important. Matrix multiplication does **not** commute; in general:\n",
    "\n",
    "$$AB \\neq BA$$\n",
    "\n",
    "This should be obvious from the fact that multiplication is only defined for matrices of dimension $p \\times q$ and $q \\times r$; unless $p=q=r$ then the multiplication is not even *defined* if the operands are switched, since it would involve a $q \\times r$ matrix by a $p \\times q$ one!\n",
    "\n",
    "Even if two matrices are compatible in dimension when permuted (i.e. if they are square matrices, so $p=q=r$), multiplication still does not generally commute and it matters which order operations are applied in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose order switching\n",
    "\n",
    "There is a very important identity which is used frequently in rearranging expressions to make computation feasible. That is:\n",
    "\n",
    "$$(AB)^T = B^TA^T$$\n",
    "    \n",
    "Remember that matrix multiplication doesn't commute, so $AB \\neq BA$ in general (though it can be true in some special cases), so this is the only way to algebraically reorder general matrix multiplication expressions (side note: inversion has the same effect, but only works on non-singular matrices). This lets us rearrange the order of matrix multiplies to \"put matrices in the right place\".\n",
    "\n",
    "It is also true that $$(A+B)^T = A^T+B^T$$ but this is less often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left-multiply and right-multiply\n",
    "Because of the noncommutativity of multiplication of matrices, there are actually two different matrix multiplication operations: **left multiplication** and **right multiplication**.\n",
    "\n",
    "$B$ left-multiply $A$ is $AB$; $B$ right-multiply $A$ is $BA$. This becomes important if we have to multiply out a longer expression:\n",
    "\n",
    "$$B\\vec{x}+\\vec{y}\\\\\n",
    "\\text{left multiply by A}\\quad =A[B\\vec{x}+\\vec{y}] = AB\\vec{x} + A\\vec{y}\\\\\n",
    "\\text{right multiply by A}\\quad =[B\\vec{x}+\\vec{y}]A = B\\vec{x}A + \\vec{y}A\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## An example matrix for measuring spread: covariance matrices\n",
    "\n",
    "As well as the **mean vector** we saw earlier, we can also generalise the idea of **variance**, which measures the spread of a dataset, to the multidimensional case. Variance (in the 1D case) is the sum of squared differences of each element from the mean of the vector:\n",
    "$$\\sigma^2 =  \\frac{1}{N-1} \\sum_{i=0}^{N-1} (x_i - \\mu_i)^2$$\n",
    "\n",
    "This is a measure of how \"spread out\" a vector of values $\\vec{x}$ is. The **standard deviation** $\\sigma$ is the square root of the **variance** and is more often used because it is in the same units as the elements of $\\vec{x}$.\n",
    "\n",
    "In the multidimensional case, to get a useful measure of spread of a $N \\times d$ data matrix  $X$ ($N$ $d$-dimensional vectors) we need to compute the *covariance* of every dimension with every other dimension. This is the average squared difference of each column of data from the average of every column. This forms a 2D array $\\Sigma$, which has entries in element $i,j$:\n",
    "\n",
    "$$\\Sigma_{ij} = \\frac{1}{N-1} \\sum_{k=1}^{N} (X_{ki}-\\mu_i)(X_{kj}-\\mu_j) $$\n",
    "\n",
    "As we will discuss shortly, this is a *special form* of matrix: it is square, symmetric and positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:18.306594Z",
     "start_time": "2020-10-01T14:08:18.299613Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0,1,(500, 5))\n",
    "\n",
    "mu = np.mean(x, axis=0)\n",
    "sigma_cross = ((x - mu).T @ (x - mu)) / (x.shape[0]-1)\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "print_matrix(\"\\Sigma_{\\\\text{cross}}\", sigma_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also directly provided by NumPy as `np.cov(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:16.570144Z",
     "start_time": "2020-10-01T14:08:16.565648Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify it is close to the function provided by NumPy\n",
    "sigma_np = np.cov(x, rowvar=False)\n",
    "print_matrix(\"\\Sigma_{\\\\text{numpy}}\", \n",
    "             sigma_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance ellipses\n",
    "This matrix captures the spread of data, including any **correlations** between dimensions. It can be seen as capturing an **ellipse** that represents a dataset.  The **mean vector** represents the centre of the ellipse, and the **covariance matrix** represent the shape of the ellipse. This ellipse is often called the **error ellipse** and is a very useful summary of high-dimensional data.\n",
    "\n",
    "The covariance matrix represents a (inverse) transform of a unit sphere to an ellipse covering the data. Sphere->ellipse is equivalent to square->parallelotope and so can be precisely represented as a matrix transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:20.904893Z",
     "start_time": "2020-10-01T14:08:20.901889Z"
    }
   },
   "outputs": [],
   "source": [
    "#import importlib; importlib.reload(utils.ellipse)\n",
    "from jhwutils import ellipse as ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:21.818973Z",
     "start_time": "2020-10-01T14:08:21.625475Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "x =  np.random.normal(0,1,(200,2)) @ np.array([[0.1, 0.5], [-0.9, 1.0]])\n",
    "\n",
    "ax.scatter(x[:,0], x[:,1], c='C0', label=\"Original\", s=10)\n",
    "ellipse.cov_ellipse(ax, x[:,0:2], 1, facecolor='none', edgecolor='k')\n",
    "ellipse.cov_ellipse(ax, x[:,0:2], 2, facecolor='none', edgecolor='k')\n",
    "ellipse.cov_ellipse(ax, x[:,0:2], 3, facecolor='none', edgecolor='k')\n",
    "\n",
    "ax.set_xlim(-4,4)\n",
    "ax.set_ylim(-4,4)\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_frame_on(False)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector and covariance matrix capture the idea of \"centre\" and \"spread\" of a collection of points in a vector space, the way the mean and the standard deviation do for real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a matrix\n",
    "The **diagonal** entries of a matrix ($A_{ii}$) are important \"landmarks\" in the structure of a matrix. Matrix elements are often referred to as being \"diagonal\" or \"off-diagonal\" terms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Matrices which are all zero except for a single diagonal entry are called... *diagonal* matrices.\n",
    "They represent a transformation that is an independent scaling of each dimension. Such matrices transform cubes to cuboids (i.e. all angles remain unchanged, and no rotation occurs).\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 3 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "`np.diag(x)` will return a diagonal matrix for a given vector `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:09:56.517282Z",
     "start_time": "2020-10-01T14:09:56.511299Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"\\\\text{diag}(x)\", np.diag([1,2,3,3,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **anti-diagonal** is the set of elements $A_{i[N-i]}$ for an $NxN$ matrix, for example a 3x3 binary anti-diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 0 & 0 & 1 \\\\\n",
    " 0 & 1 & 0 \\\\\n",
    " 1 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:10:09.850111Z",
     "start_time": "2020-10-01T14:10:09.845127Z"
    }
   },
   "outputs": [],
   "source": [
    "# an anti-diagonal array is the diagonal of the flipped array\n",
    "print_matrix(\"\\\\text{anti-diagonal}\", np.fliplr(np.diag([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Special matrix forms\n",
    "There are *many* different special types of matrices, with different properties (for example, matrices that permute dimensions, or matrices that represent *invertible* transformations). \n",
    "\n",
    "[Wikipedia's enormous list of matrices](https://en.wikipedia.org/wiki/List_of_matrices) gives a fairly comprehensive overview. We will only deal with a few special kinds of matrix in DF(H). In particular, we will deal with **real matrices** only, and primarily with **real square matrices**.\n",
    "\n",
    "\n",
    "### Identity\n",
    "The identity matrix is denoted $I$ and is a $n$ square matrix, where all values are zero except 1 along the diagonal:\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "\\dots \\\\\n",
    "0 & 0 & 0 & \\dots & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The identity matrx *has no effect* when multiplied by another matrix or vector. (Obviously, it must be dimension compatible to be multiplied at all)\n",
    "\n",
    "$IA=A=AI$ and $I{\\bf x}={\\bf x}$.\n",
    "\n",
    "It is generated by `np.eye(n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:07.161029Z",
     "start_time": "2020-10-01T14:11:07.156042Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"I\", np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:14.259191Z",
     "start_time": "2020-10-01T14:11:14.253208Z"
    }
   },
   "outputs": [],
   "source": [
    "# your identity never changes anything\n",
    "print_matrix(\"Ix\", np.eye(3) @ np.array([4,5,6]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:24.898759Z",
     "start_time": "2020-10-01T14:11:24.890781Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"AI\", np.array([[1,2,3],[4,5,6],[7,8,9]]) @ \n",
    "             np.eye(3))\n",
    "print_matrix(\"IA\", np.eye(3) @ np.array([[1,2,3],[4,5,6],[7,8,9]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any scalar multiple of the identity corresponds to a function which uniformly scales vectors:\n",
    "$$(cI){\\bf x} = c{\\bf x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:45.466948Z",
     "start_time": "2020-10-01T14:11:45.457467Z"
    }
   },
   "outputs": [],
   "source": [
    "a = 0.5\n",
    "x = np.array([4,5,6]) \n",
    "aI = np.eye(3) * a\n",
    "print(\"c=\",a)\n",
    "print_matrix(\"cI\", aI)\n",
    "print_matrix(\"(cI){\\\\bf x}\\n\", aI @ x)\n",
    "\n",
    "# the same thing:\n",
    "print_matrix(\"c{\\\\bf x}\\n\",a*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero\n",
    "The zero matrix is all zeros, and is defined for any matrix size $m\\times n$. It is written as $0$. Multiplying any vector or matrix by the zero matrix results in a result consisting of all zeros. The 0 matrix maps all vectors onto the zero vector (the origin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:58.749576Z",
     "start_time": "2020-10-01T14:11:58.745587Z"
    }
   },
   "outputs": [],
   "source": [
    "z = np.zeros((4,4))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:12:09.368510Z",
     "start_time": "2020-10-01T14:12:09.358536Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "y = np.array([[1,-1,1], [1,1,-1], [1,1,1], [-1,-1,-1]])\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"y\", y)\n",
    "print_matrix(\"0x\", z @ x)\n",
    "print()\n",
    "print_matrix(\"0y\", z @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square\n",
    "A matrix is square if it has size $n\\times n$. Square matrices are important, because they apply transformations *within* a vector space; a mapping from $n$ dimensional space to $n$ dimensional space; a map from $\\real^n \\rightarrow \\real^n$. \n",
    "\n",
    "They represent functions mapping one domain to itself. Square matrices are the only ones that:\n",
    "* have an inverse \n",
    "* have determinants\n",
    "* have an eigendecomposition\n",
    "\n",
    "which are all ideas we will see in the following unit.\n",
    "\n",
    "## Triangular\n",
    "A square matrix is triangular if it has non-zero elements only above (**upper triangular**) or below the diagonal (**lower triangular**), *inclusive of the diagonal*.\n",
    "\n",
    "**Upper triangular**\n",
    "$$\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "0 & 5 & 6 & 7 \\\\\n",
    "0 & 0 & 8 & 9 \\\\\n",
    "0 & 0 & 0 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Lower triangular**\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "2 & 3 & 0 & 0 \\\\\n",
    "4 & 5 & 6 & 0 \\\\\n",
    "7 & 8 & 9 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "These represent particularly simple to solve sets of simultaneous equations. For example, the lower triangular matrix above can be seen as the system of equations:\n",
    "\n",
    "$$x_1 = y_1\\\\\n",
    "2x_1 + 3x_2 = y_2\\\\ \n",
    "4x_1 + 5x_2 + 6x_3 = y_3\\\\ \n",
    "7x_1 + 8x_2 + 9x_3 + 10x_4 = y_4\\\\ \n",
    "$$\n",
    "\n",
    "which, for a given $y_1, y_2, y_3, y_4$ is trivial to solve by substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:13:36.995921Z",
     "start_time": "2020-10-01T14:13:36.987436Z"
    }
   },
   "outputs": [],
   "source": [
    "# tri generates an all ones lower triangular matrix\n",
    "upper = np.tri(4) \n",
    "print_matrix(\"T_u\", upper)\n",
    "\n",
    "# transpose changes a lower triangular to an upper triangular\n",
    "lower = np.tri(4).T \n",
    "print_matrix(\"T_l\", lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "* [**3blue1brown Linear Algebra series**](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  (**strongly recommended**)\n",
    "\n",
    "* [**Introduction to applied linear algebra**](http://stanford.edu/~boyd/vmls/vmls.pdf ) *by S. Boyd and L. Vandenberghe* \n",
    " \n",
    " \n",
    "## Beyond this course\n",
    "* **Linear Algebra Done Right** *by Sheldon Axler* (excellent introduction to the \"pure math\" side of linear algebra) ISBN-13: 978-0387982588\n",
    "\n",
    "* **Coding the Matrix: Linear Algebra through Applications to Computer Science** *by Philip N Klein* (top quality textbook on how linear algebra is implemented, all in Python) ISBN-13: 978-0615880990\n",
    "\n",
    "* **Linear Algebra and Learning from Data** *Gilbert Strang*, ISBN-13: 978-069219638-0, explains many detailed aspects of linear algebra and how they relate to data science.\n",
    "\n",
    "## Way beyond this course\n",
    "* [**The Matrix Cookbook**](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) *by Kaare Brandt Petersen and Michael Syskind Pedersen*. If you need to do a tricky calculation with matrices, this book will probably tell you how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs as matrices\n",
    "\n",
    "## Example: distributing packages\n",
    "\n",
    "<img src=\"imgs/distribution.jpg\" width=\"50%\"> <br>\n",
    "*.[[Image](https://flickr.com/photos/nseika/8096899965 \"P1070904_DxO\") by [nSeika](https://flickr.com/people/nseika) shared [CC BY](https://creativecommons.org/licenses/by/2.0/)] .*\n",
    "\n",
    "You run a large logistics company. You have to route packages between distributions centres efficiently, so they will be ready for local delivery. To do this, you need to be able to predict which warehouses are going to receive lots of packages (maybe they are connected to other sites by several direct motorways) and which will receive few packages (maybe they are remote).\n",
    "\n",
    "How can this problem be modelled? If we can make the assumption that the flow from site to site is **linear** -- that the packages arriving at one site is a weighted sum of the packages currently at each of the other sites -- then we can model the problem with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might model the connectivity of distribution centres as a **graph**.\n",
    "A **directed graph** connects **vertices** by **edges**. \n",
    "The definition of a graph is $G=(V,E)$, where $V$ is a set of vertices and $E$ is a set of edges connecting pairs of vertices.\n",
    "\n",
    "<img src=\"imgs/graph.png\">\n",
    "\n",
    "The graph above has 8 vertices $(A,B,C,D,E,F,G,H)$ and 11 edges:\n",
    "\n",
    "$$    A \\rightarrow B \\\\\n",
    "      A \\rightarrow C \\\\\n",
    "      A \\rightarrow D \\\\\n",
    "      B \\rightarrow D \\\\\n",
    "      B \\rightarrow E \\\\\n",
    "      C \\rightarrow B \\\\\n",
    "      C \\rightarrow D \\\\\n",
    "      D \\rightarrow F \\\\\n",
    "      D \\rightarrow G \\\\\n",
    "      D \\rightarrow H \\\\\n",
    "      H \\rightarrow A $$\n",
    "\n",
    "We can write this as an **adjacency matrix**. We number each vertex $0, 1, 2, 3, \\dots$. We then create a square matrix $A$ whose elements are all zero, except where there is an edge from $V_i$ to $V_j$, in which case we set $A_{ij} = 1$. The graph shown above has the adjacency matrix:\n",
    "\n",
    "         A B C D E F G H\n",
    "      \n",
    "     A   0 1 1 1 0 0 0 0\n",
    "     B   0 0 0 1 1 0 0 0\n",
    "     C   0 1 0 1 0 0 0 0\n",
    "     D   0 0 0 0 0 1 1 1\n",
    "     E   0 0 0 0 0 0 0 0\n",
    "     F   0 0 0 0 0 0 0 0 \n",
    "     G   0 0 0 0 0 0 0 0\n",
    "     H   1 0 0 0 0 0 0 0\n",
    "\n",
    "(the letters aren't part of the matrix and are just shown for clarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing graph properties\n",
    "\n",
    "There are some graph properties which we can compute easily from this binary matrix:\n",
    "* The *out-degree* of each vertex (number of edges leaving a vertex) is the sum of each row.\n",
    "* The *in-degree* of each vertex (number of edges entering a vertex) is the sum of each column.\n",
    "* If the matrix is symmetric it represents an undirected graph; this is the case if it is equal to its transpose.\n",
    "* A directed graph can be converted to an undirected graph by computing $A^\\prime = A + A^T$. This is equivalent to making all the arrows bi-directional.\n",
    "* If there are non-zero elements on the diagonal, that means there are edges connecting vertices to themselves (self-transitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T12:10:11.073498Z",
     "start_time": "2020-10-08T12:10:10.942208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our adjacency matrix:\n",
    "adj = np.array([[0, 1, 1, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                [0, 1, 0, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 1, 1, 1],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0], \n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# compute in-degrees and out-degrees\n",
    "in_degrees = np.sum(adj, axis=0)\n",
    "out_degrees = np.sum(adj, axis=1)\n",
    "print('In degrees: ', list(zip('ABCDEFGH', in_degrees)))\n",
    "print('Out degrees:', list(zip('ABCDEFGH', out_degrees)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T12:10:24.455633Z",
     "start_time": "2020-10-08T12:10:24.242832Z"
    }
   },
   "outputs": [],
   "source": [
    "# is the graph undirected?\n",
    "print(np.allclose(adj, adj.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T12:10:41.118022Z",
     "start_time": "2020-10-08T12:10:41.087766Z"
    }
   },
   "outputs": [],
   "source": [
    "# if we want to *force* our adjacency matrix to be symmetric,\n",
    "# i.e. convert the graph from directed to undirected, \n",
    "# we can add it to its transpose\n",
    "adj_sym = adj + adj.T\n",
    "print(adj_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T12:11:03.220396Z",
     "start_time": "2020-10-08T12:11:03.209660Z"
    }
   },
   "outputs": [],
   "source": [
    "# any self transitions?\n",
    "print(np.all(np.diag(adj)==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [**Introduction to Linear Algebra**](http://math.mit.edu/~gs/linearalgebra/) by Gilbert Strang. The standard reference text book for linear algebra.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "format_version": "1.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
